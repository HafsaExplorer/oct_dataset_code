{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fb7fbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D , SeparableConv2D, MaxPooling2D , Flatten , Dropout , BatchNormalization, Activation\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import img_to_array,array_to_img\n",
    "from keras.callbacks import ReduceLROnPlateau \n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from sklearn.metrics import classification_report, recall_score, precision_score, confusion_matrix, f1_score, accuracy_score\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from IPython.display import Image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f428655",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = 'E:/oct_dataset/oct_dataset/data_final_all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54071fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 66788 images belonging to 4 classes.\n",
      "Found 16696 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Generate training data\n",
    "train_gen = train_datagen.flow_from_directory(\n",
    "    directory=train_data_dir,\n",
    "    target_size=(224, 224),  # Adjust the target size according to your requirements\n",
    "    batch_size=16,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "# Generate validation data\n",
    "val_gen = train_datagen.flow_from_directory(\n",
    "    directory=train_data_dir,\n",
    "    target_size=(224, 224),  # Adjust the target size according to your requirements\n",
    "    batch_size=16,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd6e93b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (224, 224, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26fa3451",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\PMLS\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\PMLS\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_model = tf.keras.applications.ResNet152V2(input_shape=image_size,\n",
    "                                               include_top=False,\n",
    "                                               weights=\"imagenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1adc29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\PMLS\\anaconda3\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.logging.TaskLevelStatusMessage is deprecated. Please use tf.compat.v1.logging.TaskLevelStatusMessage instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\PMLS\\anaconda3\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import Metric\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "class F1Score(Metric):\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        super(F1Score, self).__init__(name=name, **kwargs)\n",
    "        self.true_positives = self.add_weight(name='true_positives', initializer='zeros')\n",
    "        self.possible_positives = self.add_weight(name='possible_positives', initializer='zeros')\n",
    "        self.predicted_positives = self.add_weight(name='predicted_positives', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        true_positives = tf.reduce_sum(y_true * y_pred)\n",
    "        possible_positives = tf.reduce_sum(y_true)\n",
    "        predicted_positives = tf.reduce_sum(y_pred)\n",
    "        self.true_positives.assign_add(true_positives)\n",
    "        self.possible_positives.assign_add(possible_positives)\n",
    "        self.predicted_positives.assign_add(predicted_positives)\n",
    "\n",
    "    def result(self):\n",
    "        precision = self.true_positives / (self.predicted_positives + tf.keras.backend.epsilon())\n",
    "        recall = self.true_positives / (self.possible_positives + tf.keras.backend.epsilon())\n",
    "        f1 = 2 * precision * recall / (precision + recall + tf.keras.backend.epsilon())\n",
    "        return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bbb8c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianConv2D(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, activation=None, **kwargs):\n",
    "        super(BayesianConv2D, self).__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        self.conv2d = tf.keras.layers.Conv2D(filters=filters, kernel_size=kernel_size, activation=activation, **kwargs)\n",
    "        self.conv2d_flipout = tfp.layers.Convolution2DFlipout(filters=filters, kernel_size=kernel_size, activation=activation, **kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv2d(inputs)\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return self.conv2d.compute_output_shape(input_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(BayesianConv2D, self).get_config()\n",
    "        config.update({\n",
    "            'filters': self.filters,\n",
    "            'kernel_size': self.kernel_size,\n",
    "            'activation': self.activation,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "311a566b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianDense(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super(BayesianDense, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        self.dense_flipout = tfp.layers.DenseFlipout(units, activation=activation, **kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_flipout(inputs)\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return self.dense_flipout.compute_output_shape(input_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(BayesianDense, self).get_config()\n",
    "        config.update({\n",
    "            'units': self.units,\n",
    "            'activation': self.activation,\n",
    "        })\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "221f08c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PMLS\\anaconda3\\Lib\\site-packages\\tensorflow_probability\\python\\layers\\util.py:98: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  loc = add_variable_fn(\n",
      "C:\\Users\\PMLS\\anaconda3\\Lib\\site-packages\\tensorflow_probability\\python\\layers\\util.py:108: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  untransformed_scale = add_variable_fn(\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(224, 224, 3)),\n",
    "    BayesianConv2D(32, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "    BayesianConv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "    BayesianConv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "    Flatten(),\n",
    "    BayesianDense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    BayesianDense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(4, activation='softmax')  # Adjust output units based on the number of classes\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ed0560c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=CategoricalCrossentropy(), optimizer=Adam(), metrics=['accuracy', F1Score()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83edc39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop = EarlyStopping(verbose=1, patience=5)\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', patience=2, verbose=1, factor=0.8, min_lr=1e-6)\n",
    "checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20cd32e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4175/4175 [==============================] - 9285s 2s/step - loss: 17824398.0000 - accuracy: 0.0000e+00 - f1_score: 0.2047 - val_loss: 181664.0312 - val_accuracy: 0.0000e+00 - val_f1_score: 0.1613 - lr: 0.0010\n",
      "4175/4175 [==============================] - 42417s 10s/step - loss: 2620.0015 - accuracy: 0.0000e+00 - f1_score: 0.1818 - val_loss: 1.2559 - val_accuracy: 0.0000e+00 - val_f1_score: 0.1556 - lr: 0.0010\n",
      "4175/4175 [==============================] - 13644s 3s/step - loss: 1.2907 - accuracy: 0.0000e+00 - f1_score: 0.1738 - val_loss: 1.2558 - val_accuracy: 0.0000e+00 - val_f1_score: 0.1545 - lr: 0.0010\n",
      "4175/4175 [==============================] - 11197s 3s/step - loss: 1.2716 - accuracy: 0.0000e+00 - f1_score: 0.1629 - val_loss: 1.2468 - val_accuracy: 0.0000e+00 - val_f1_score: 0.1543 - lr: 0.0010\n",
      "4175/4175 [==============================] - ETA: 0s - loss: 1.2637 - accuracy: 0.0000e+00 - f1_score: 0.1573"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 16\u001b[0m\n\u001b[0;32m     10\u001b[0m     lr \u001b[38;5;241m=\u001b[39m icda_strategy(epoch)\n\u001b[0;32m     11\u001b[0m     model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m     12\u001b[0m         loss\u001b[38;5;241m=\u001b[39mCategoricalCrossentropy(),\n\u001b[0;32m     13\u001b[0m         optimizer\u001b[38;5;241m=\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39mlr),\n\u001b[0;32m     14\u001b[0m         metrics\u001b[38;5;241m=\u001b[39m[Accuracy(), tfa\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mF1Score(num_classes\u001b[38;5;241m=\u001b[39mnum_classes, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[0;32m     15\u001b[0m     )\n\u001b[1;32m---> 16\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     17\u001b[0m         train_gen,\n\u001b[0;32m     18\u001b[0m         validation_data\u001b[38;5;241m=\u001b[39mval_gen,\n\u001b[0;32m     19\u001b[0m         epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     20\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     21\u001b[0m             EarlyStopping(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m     22\u001b[0m             ReduceLROnPlateau(factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     23\u001b[0m             ModelCheckpoint(filepath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_checkpoint.keras\u001b[39m\u001b[38;5;124m'\u001b[39m, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     24\u001b[0m         ]\n\u001b[0;32m     25\u001b[0m     )\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Ensure that the RandomNormal initializer is seeded.\u001b[39;00m\n\u001b[0;32m     28\u001b[0m initializer \u001b[38;5;241m=\u001b[39m RandomNormal(mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, stddev\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\h5py\\_hl\\group.py:483\u001b[0m, in \u001b[0;36mGroup.__setitem__\u001b[1;34m(self, name, obj)\u001b[0m\n\u001b[0;32m    480\u001b[0m     htype\u001b[38;5;241m.\u001b[39mcommit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid, name, lcpl\u001b[38;5;241m=\u001b[39mlcpl)\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 483\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_dataset(\u001b[38;5;28;01mNone\u001b[39;00m, data\u001b[38;5;241m=\u001b[39mobj)\n\u001b[0;32m    484\u001b[0m     h5o\u001b[38;5;241m.\u001b[39mlink(ds\u001b[38;5;241m.\u001b[39mid, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid, name, lcpl\u001b[38;5;241m=\u001b[39mlcpl)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\h5py\\_hl\\group.py:183\u001b[0m, in \u001b[0;36mGroup.create_dataset\u001b[1;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[0;32m    180\u001b[0m         parent_path, name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    181\u001b[0m         group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequire_group(parent_path)\n\u001b[1;32m--> 183\u001b[0m dsid \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmake_new_dset(group, shape, dtype, data, name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    184\u001b[0m dset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mDataset(dsid)\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dset\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\h5py\\_hl\\dataset.py:166\u001b[0m, in \u001b[0;36mmake_new_dset\u001b[1;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter, rdcc_nslots, rdcc_nbytes, rdcc_w0)\u001b[0m\n\u001b[0;32m    163\u001b[0m dset_id \u001b[38;5;241m=\u001b[39m h5d\u001b[38;5;241m.\u001b[39mcreate(parent\u001b[38;5;241m.\u001b[39mid, name, tid, sid, dcpl\u001b[38;5;241m=\u001b[39mdcpl, dapl\u001b[38;5;241m=\u001b[39mdapl)\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Empty)):\n\u001b[1;32m--> 166\u001b[0m     dset_id\u001b[38;5;241m.\u001b[39mwrite(h5s\u001b[38;5;241m.\u001b[39mALL, h5s\u001b[38;5;241m.\u001b[39mALL, data)\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dset_id\n",
      "File \u001b[1;32mh5py\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\h5d.pyx:283\u001b[0m, in \u001b[0;36mh5py.h5d.DatasetID.write\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\_proxy.pyx:114\u001b[0m, in \u001b[0;36mh5py._proxy.dset_rw\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\h5fd.pyx:176\u001b[0m, in \u001b[0;36mh5py.h5fd.H5FD_fileobj_write\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Accuracy\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "\n",
    "num_classes = 4\n",
    "\n",
    "for epoch in range(10):\n",
    "    lr = icda_strategy(epoch)\n",
    "    model.compile(\n",
    "        loss=CategoricalCrossentropy(),\n",
    "        optimizer=Adam(learning_rate=lr),\n",
    "        metrics=[Accuracy(), tfa.metrics.F1Score(num_classes=num_classes, average='macro')]\n",
    "    )\n",
    "    model.fit(\n",
    "        train_gen,\n",
    "        validation_data=val_gen,\n",
    "        epochs=1,\n",
    "        callbacks=[\n",
    "            EarlyStopping(patience=10, monitor='val_loss', restore_best_weights=True),\n",
    "            ReduceLROnPlateau(factor=0.5, patience=5, monitor='val_loss'),\n",
    "            ModelCheckpoint(filepath='model_checkpoint.keras', monitor='val_loss', save_best_only=True)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# Ensure that the RandomNormal initializer is seeded.\n",
    "initializer = RandomNormal(mean=0.0, stddev=0.05, seed=42)\n",
    "\n",
    "# Update any custom layer or model class to use 'add_weight' instead of 'add_variable'.\n",
    "# Example of defining a custom layer\n",
    "class CustomLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CustomLayer, self).__init__(**kwargs)\n",
    "        self.loc = self.add_weight(name='loc', i2nitializer=initializer, shape=(1,), dtype=tf.float32) \n",
    "        self.untransformed_scale = self.add_weight(name='untransformed_scale', initializer=initializer, shape=(1,), dtype=tf.float32)\n",
    "\n",
    "# If you have a custom PyDataset or similar class, ensure it calls super().__init__.\n",
    "class PyDataset:\n",
    "    def __init__(self, **kwargs):\n",
    "        super(PyDataset, self).__init__(**kwargs)\n",
    "        # Your initialization code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbad461",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        horizontal_flip = True,\n",
    "        )\n",
    "\n",
    "complete_generator = complete_datagen.flow_from_directory(\n",
    "        directory = train_data_dir,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=1,\n",
    "        class_mode=None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf54f7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19903/83484\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:54:37\u001b[0m 391ms/step"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate the number of steps needed to cover the entire dataset\n",
    "STEP_SIZE_COMPLETE = complete_generator.n // complete_generator.batch_size\n",
    "\n",
    "# Predict class probabilities for each sample in the dataset\n",
    "train_probs = model.predict(complete_generator, steps=STEP_SIZE_COMPLETE)\n",
    "\n",
    "# Convert predicted probabilities to class labels (0, 1, 2, or 3)\n",
    "train_preds = np.argmax(train_probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "975ecb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 3}\n"
     ]
    }
   ],
   "source": [
    "print(set(train_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28dbdb7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3]), array([37205, 11348,  8616, 26315], dtype=int64))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(complete_generator.classes, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3725f3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 -> 83479\n",
      "Class 1 -> 0\n",
      "Class 2 -> 0\n",
      "Class 3 -> 5\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    value = train_preds.count(i)\n",
    "    print(f'Class {i} -> {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69264546",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
